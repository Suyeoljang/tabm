"""
TabM ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (Target Encoding ì§€ì›)
í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ CSV ë°ì´í„°ì˜ PROC_EXPOSE_LOG ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import torch
import pickle
import argparse
from pathlib import Path
from typing import NamedTuple

try:
    import tabm
    import rtdl_num_embeddings
except ImportError:
    print("âš ï¸  tabm íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)


# RegressionLabelStats ì •ì˜ (ëª¨ë¸ ë¡œë“œ ì‹œ í•„ìš”)
class RegressionLabelStats(NamedTuple):
    mean: float
    std: float


# PyTorch 2.6 í˜¸í™˜ì„±: safe globalsì— ì¶”ê°€
try:
    torch.serialization.add_safe_globals([RegressionLabelStats])
except AttributeError:
    pass

# ================================================================
# ì„¤ì •
# ================================================================

def parse_args():
    parser = argparse.ArgumentParser(description='TabM ëª¨ë¸ ì¶”ë¡ ')
    parser.add_argument('--input_csv', type=str,
                        default='testset_1126_analyized.csv',
                        help='ì˜ˆì¸¡í•  CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model_path', type=str, 
                        default='tabm_model_fixed.pt',
                        help='í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--preprocessing_meta', type=str,
                        default='preprocessing_metadata.pkl',
                        help='ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output_csv', type=str, 
                        default='predictions.csv',
                        help='ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--batch_size', type=int, default=8192,
                        help='ì¶”ë¡  ë°°ì¹˜ í¬ê¸°')
    
    return parser.parse_args()


# ================================================================
# ì¸ì½”ë”© í•¨ìˆ˜ ì¶”ê°€
# ================================================================

def apply_frequency_encoding(df, col, freq_info):
    """Frequency Encoding ì ìš©"""
    freq_dict = freq_info['freq_dict']
    return df[col].map(freq_dict).fillna(0).values

def apply_target_encoding(df, col, target_info):
    """Target Encoding ì ìš© (inference)"""
    mean_dict = target_info['mean_dict']
    global_mean = target_info['global_mean']
    return df[col].map(mean_dict).fillna(global_mean).values


# ================================================================
# ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜
# ================================================================

def predict(input_csv_path, model_path, preprocessing_meta_path, output_csv_path, batch_size=8192):
    """
    í•™ìŠµëœ TabM ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
    
    Args:
        input_csv_path: ì˜ˆì¸¡í•  CSV íŒŒì¼ ê²½ë¡œ
        model_path: í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        preprocessing_meta_path: ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        output_csv_path: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        batch_size: ì¶”ë¡  ë°°ì¹˜ í¬ê¸°
    """
    
    print("=" * 70)
    print("TabM ëª¨ë¸ ì¶”ë¡  (ìµœì  ì¸ì½”ë”© ì „ëµ)")
    print("=" * 70)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\në””ë°”ì´ìŠ¤: {device}")
    
    # ================================================================
    # 1. ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
    # ================================================================
    print("\n" + "=" * 70)
    print("1. ëª¨ë¸ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ")
    print("=" * 70)
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    regression_label_stats = checkpoint['regression_label_stats']
    preprocessing = checkpoint['preprocessing']
    label_encoders = checkpoint['label_encoders']
    numerical_cols = checkpoint['numerical_cols']
    categorical_cols = checkpoint['categorical_cols']
    cat_cardinalities = checkpoint['cat_cardinalities']
    config = checkpoint.get('config', {})
    
    # ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìƒˆë¡œìš´ ì¸ì½”ë”© ì „ëµ)
    with open(preprocessing_meta_path, 'rb') as f:
        preprocessing_meta = pickle.load(f)
    
    encoding_info = preprocessing_meta.get('encoding_info', {})
    encoding_strategy = preprocessing_meta.get('encoding_strategy', {})
    
    # ì›ë³¸ ë²”ì£¼í˜• ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    # original_categorical_colsê°€ ë©”íƒ€ë°ì´í„°ì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ êµ¬ì„±
    if 'original_categorical_cols' in preprocessing_meta:
        original_categorical_cols = preprocessing_meta['original_categorical_cols']
    else:
        # categorical_cols + very_high_card_cols
        original_categorical_cols = (
            categorical_cols + 
            encoding_strategy.get('very_high_card', [])
        )
    
    very_high_card_cols = encoding_strategy.get('very_high_card', [])
    high_card_cols = encoding_strategy.get('high_card', [])
    medium_card_cols = encoding_strategy.get('medium_card', [])
    low_card_cols = encoding_strategy.get('low_card', [])
    
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ: {model_path}")
    print(f"  íƒ€ê²Ÿ í‰ê· : {regression_label_stats.mean:.6f}")
    print(f"  íƒ€ê²Ÿ í‘œì¤€í¸ì°¨: {regression_label_stats.std:.6f}")
    print(f"  ì—°ì†í˜• ë³€ìˆ˜: {len(numerical_cols)}ê°œ")
    print(f"  ë²”ì£¼í˜• ë³€ìˆ˜: {len(categorical_cols)}ê°œ")
    
    print(f"\n  ì¸ì½”ë”© ì „ëµ:")
    print(f"    Very High Card (>300):  {len(very_high_card_cols)}ê°œ")
    print(f"    High Card (151-300):    {len(high_card_cols)}ê°œ")
    print(f"    Medium Card (51-150):   {len(medium_card_cols)}ê°œ")
    print(f"    Low Card (â‰¤50):         {len(low_card_cols)}ê°œ")
    
    # ================================================================
    # 2. ì…ë ¥ ë°ì´í„° ë¡œë“œ
    # ================================================================
    print("\n" + "=" * 70)
    print("2. ì…ë ¥ ë°ì´í„° ë¡œë“œ")
    print("=" * 70)
    
    df_input = pd.read_csv(input_csv_path)
    print(f"âœ“ ì…ë ¥ ë°ì´í„°: {df_input.shape}")
    print(f"  íŒŒì¼: {input_csv_path}")
    
    # ì›ë³¸ ë°ì´í„° ë°±ì—…
    df_original = df_input.copy()
    
    # ================================================================
    # í•„ìˆ˜: í•™ìŠµì— ì‚¬ìš©ëœ ì›ë³¸ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
    # ================================================================
    # ì›ë³¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì¸ì½”ë”© ì „)
    original_numerical_cols = [col for col in numerical_cols 
                              if not any(col.endswith(suffix) for suffix in ['_target', '_freq'])]
    
    # í•„ìš”í•œ ëª¨ë“  ì›ë³¸ ì»¬ëŸ¼
    required_original_cols = list(set(original_numerical_cols + original_categorical_cols))
    
    # ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
    missing_cols = [col for col in required_original_cols if col not in df_input.columns]
    
    if missing_cols:
        print(f"\nâš ï¸  ê²½ê³ : ë‹¤ìŒ ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤:")
        for col in missing_cols:
            print(f"    - {col}")
        print(f"\nëˆ„ë½ëœ ì»¬ëŸ¼ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
        
        for col in missing_cols:
            if col in original_numerical_cols:
                df_input[col] = 0.0
            else:
                df_input[col] = 'UNKNOWN'
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°)
    extra_cols = [col for col in df_input.columns if col not in required_original_cols]
    if extra_cols:
        print(f"\nğŸ“Œ ì…ë ¥ ë°ì´í„°ì—ì„œ {len(extra_cols)}ê°œì˜ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°")
        print(f"   í•„ìš”í•œ ì»¬ëŸ¼: {len(required_original_cols)}ê°œ")
        df_input = df_input[required_original_cols].copy()
    
    print(f"\nâœ“ ì›ë³¸ ì»¬ëŸ¼ ì¶”ì¶œ ì™„ë£Œ")
    print(f"  ìˆ˜ì¹˜í˜•: {len(original_numerical_cols)}ê°œ")
    print(f"  ë²”ì£¼í˜•: {len(original_categorical_cols)}ê°œ")
    
    # ================================================================
    # 3. ë°ì´í„° ì „ì²˜ë¦¬ (ìƒˆë¡œìš´ ì¸ì½”ë”© ì „ëµ ì ìš©)
    # ================================================================
    print("\n" + "=" * 70)
    print("3. ë°ì´í„° ì „ì²˜ë¦¬")
    print("=" * 70)
    
    # 3-1. Very High Cardinality (>300): K-Fold Target Encodingë§Œ
    if very_high_card_cols:
        print(f"\nVery High Cardinality ë³€ìˆ˜ ì²˜ë¦¬ (K-Fold Target Encoding):")
        
        for col in very_high_card_cols:
            if col not in df_input.columns:
                print(f"  âš ï¸  ê²½ê³ : {col} ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            if col in encoding_info and encoding_info[col]['type'] == 'kfold_target':
                info = encoding_info[col]
                
                # K-Fold Target Encoding ì ìš©
                target_encoded = apply_target_encoding(df_input, col, info['target_info'])
                new_col = f'{col}_target'
                df_input[new_col] = target_encoded
                
                print(f"  âœ“ {col} â†’ {new_col}")
                print(f"    í•™ìŠµëœ ì¹´í…Œê³ ë¦¬: {len(info['target_info']['mean_dict'])}ê°œ")
                
                # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
                df_input = df_input.drop(columns=[col])
    
    # 3-2. High Cardinality (151-300): Label + Frequency + Target
    if high_card_cols:
        print(f"\nHigh Cardinality ë³€ìˆ˜ ì²˜ë¦¬ (Label + Freq + Target):")
        
        for col in high_card_cols:
            if col not in df_input.columns:
                print(f"  âš ï¸  ê²½ê³ : {col} ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            if col in encoding_info and encoding_info[col]['type'] == 'high_card':
                info = encoding_info[col]
                
                # Frequency Encoding
                freq_encoded = apply_frequency_encoding(df_input, col, info['freq_info'])
                df_input[f'{col}_freq'] = freq_encoded
                
                # Target Encoding
                target_encoded = apply_target_encoding(df_input, col, info['target_info'])
                df_input[f'{col}_target'] = target_encoded
                
                print(f"  âœ“ {col} â†’ _freq, _target")
    
    # 3-3. Medium Cardinality (51-150): Label + Frequency
    if medium_card_cols:
        print(f"\nMedium Cardinality ë³€ìˆ˜ ì²˜ë¦¬ (Label + Freq):")
        
        for col in medium_card_cols:
            if col not in df_input.columns:
                print(f"  âš ï¸  ê²½ê³ : {col} ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            if col in encoding_info and encoding_info[col]['type'] == 'medium_card':
                info = encoding_info[col]
                
                # Frequency Encoding
                freq_encoded = apply_frequency_encoding(df_input, col, info['freq_info'])
                df_input[f'{col}_freq'] = freq_encoded
                
                print(f"  âœ“ {col} â†’ _freq")
    
    # 3-4. Low Cardinality: Label Only (ë³„ë„ ì²˜ë¦¬ ì—†ìŒ)
    if low_card_cols:
        print(f"\nLow Cardinality ë³€ìˆ˜: Label Encodingë§Œ ì ìš© ({len(low_card_cols)}ê°œ)")
    
    # 3-5. ë²”ì£¼í˜• ë³€ìˆ˜ Label Encoding
    print(f"\në²”ì£¼í˜• ë³€ìˆ˜ Label Encoding:")
    
    for col in categorical_cols:
        if col not in df_input.columns:
            print(f"  âš ï¸  ê²½ê³ : {col} ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. 'UNKNOWN'ìœ¼ë¡œ ì„¤ì •")
            df_input[col] = 'UNKNOWN'
            
        if col in label_encoders:
            le = label_encoders[col]
            
            def safe_transform(x):
                try:
                    return le.transform([str(x)])[0]
                except ValueError:
                    return len(le.classes_)  # UNKNOWN
            
            df_input[col] = df_input[col].astype(str).apply(safe_transform)
    
    print(f"âœ“ ë²”ì£¼í˜• ì¸ì½”ë”© ì™„ë£Œ")
    
    # 3-6. ëˆ„ë½ëœ ì—°ì†í˜• ì»¬ëŸ¼ í™•ì¸ ë° ì±„ìš°ê¸°
    for col in numerical_cols:
        if col not in df_input.columns:
            print(f"  âš ï¸  ê²½ê³ : {col} ì»¬ëŸ¼ì´ ì…ë ¥ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            df_input[col] = 0.0
    
    # 3-7. numpy ë°°ì—´ ë³€í™˜
    X_num = df_input[numerical_cols].values.astype(np.float32)
    X_cat = df_input[categorical_cols].values.astype(np.int64)
    
    # 3-8. ì—°ì†í˜• ë³€ìˆ˜ ì •ê·œí™”
    X_num = preprocessing.transform(X_num)
    
    print(f"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ")
    print(f"  ì—°ì†í˜•: {X_num.shape}")
    print(f"  ë²”ì£¼í˜•: {X_cat.shape}")
    
    # ================================================================
    # 4. ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    # ================================================================
    print("\n" + "=" * 70)
    print("4. ëª¨ë¸ ìƒì„±")
    print("=" * 70)
    
    n_num_features = len(numerical_cols)
    n_cat_features = len(categorical_cols)
    
    # configì—ì„œ n_bins, d_embeddings ê°€ì ¸ì˜¤ê¸°
    n_bins = config.get('n_bins', 24)
    d_embeddings = config.get('d_embeddings', 32)
    dropout = config.get('dropout', 0.1)
    
    print(f"  n_bins: {n_bins}")
    print(f"  d_embeddings: {d_embeddings}")
    print(f"  dropout: {dropout}")
    
    # bins ê³„ì‚°ì„ ìœ„í•œ ëœë¤ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X_num_for_bins = np.random.randn(1000, n_num_features).astype(np.float32)
    
    # ì‹¤ì œ ë°ì´í„°ì˜ ë²”ìœ„ì— ë§ì¶° ìŠ¤ì¼€ì¼ ì¡°ì •
    n_samples = len(X_num)
    if n_samples > 0:
        data_min = X_num.min(axis=0)
        data_max = X_num.max(axis=0)
        data_range = data_max - data_min
        data_range[data_range == 0] = 1.0
        X_num_for_bins = X_num_for_bins * data_range + data_min
    
    # Num embeddings ìƒì„±
    num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(
        rtdl_num_embeddings.compute_bins(
            torch.tensor(X_num_for_bins, device='cpu'),
            n_bins=n_bins
        ),
        d_embedding=d_embeddings,
        activation=False,
        version='B',
    )
    
    model = tabm.TabM.make(
        n_num_features=n_num_features,
        cat_cardinalities=cat_cardinalities,
        d_out=1,
        num_embeddings=num_embeddings,
        dropout=dropout,
    ).to(device)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    model.eval()
    
    print(f"âœ“ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"  íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ================================================================
    # 5. ì˜ˆì¸¡
    # ================================================================
    print("\n" + "=" * 70)
    print("5. ì˜ˆì¸¡ ì¤‘...")
    print("=" * 70)
    
    # PyTorch í…ì„œ ë³€í™˜
    X_num_tensor = torch.tensor(X_num, device=device)
    X_cat_tensor = torch.tensor(X_cat, device=device)
    
    predictions_list = []
    
    with torch.no_grad():
        n_samples = len(X_num)
        
        for i in range(0, n_samples, batch_size):
            batch_end = min(i + batch_size, n_samples)
            
            x_num_batch = X_num_tensor[i:batch_end]
            x_cat_batch = X_cat_tensor[i:batch_end]
            
            # ì˜ˆì¸¡
            y_pred_batch = model(x_num_batch, x_cat_batch)
            
            # (batch_size, k, 1) â†’ (batch_size,)
            y_pred_batch = y_pred_batch.squeeze(-1).mean(dim=1)
            
            predictions_list.append(y_pred_batch.cpu().numpy())
            
            if (i // batch_size + 1) % 10 == 0 or batch_end == n_samples:
                print(f"  ì§„í–‰: {batch_end}/{n_samples} ({batch_end/n_samples*100:.1f}%)")
    
    # ì „ì²´ ì˜ˆì¸¡ ê²°í•©
    predictions = np.concatenate(predictions_list)
    
    # ì—­í‘œì¤€í™” (ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ)
    predictions = predictions * regression_label_stats.std + regression_label_stats.mean
    
    print(f"\nâœ“ ì˜ˆì¸¡ ì™„ë£Œ!")
    print(f"  ì˜ˆì¸¡ê°’ ë²”ìœ„: [{predictions.min():.6f}, {predictions.max():.6f}]")
    print(f"  ì˜ˆì¸¡ê°’ í‰ê· : {predictions.mean():.6f}")
    print(f"  ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨: {predictions.std():.6f}")
    
    # ================================================================
    # 6. ê²°ê³¼ ì €ì¥
    # ================================================================
    print("\n" + "=" * 70)
    print("6. ê²°ê³¼ ì €ì¥")
    print("=" * 70)
    
    # ì›ë³¸ ë°ì´í„°ì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    df_result = df_original.copy()
    df_result['PROC_EXPOSE_LOG_PRED'] = predictions
    
    # exp ë³€í™˜ (ë¡œê·¸ì˜€ë‹¤ë©´)
    df_result['PROC_EXPOSE_PRED'] = np.expm1(predictions)
    
    # CSV ì €ì¥
    df_result.to_csv(output_csv_path, index=False)
    
    print(f"âœ“ ê²°ê³¼ ì €ì¥: {output_csv_path}")
    print(f"  ì»¬ëŸ¼: {list(df_result.columns)}")
    print(f"  í–‰ ìˆ˜: {len(df_result)}")
    
    # ================================================================
    # 7. ìš”ì•½ í†µê³„
    # ================================================================
    print("\n" + "=" * 70)
    print("7. ì˜ˆì¸¡ ìš”ì•½")
    print("=" * 70)
    
    print(f"\nì˜ˆì¸¡ê°’ í†µê³„:")
    print(f"  ìµœì†Ÿê°’:   {predictions.min():.6f}")
    print(f"  ìµœëŒ“ê°’:   {predictions.max():.6f}")
    print(f"  í‰ê· :     {predictions.mean():.6f}")
    print(f"  ì¤‘ì•™ê°’:   {np.median(predictions):.6f}")
    print(f"  í‘œì¤€í¸ì°¨: {predictions.std():.6f}")
    
    # ë°±ë¶„ìœ„ìˆ˜
    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
    print(f"\në°±ë¶„ìœ„ìˆ˜:")
    for p in percentiles:
        val = np.percentile(predictions, p)
        print(f"  {p:2d}%: {val:.6f}")
    
    # ì‹¤ì œê°’ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'PROC_EXPOSE_LOG' in df_original.columns:
        print("\n" + "=" * 70)
        print("8. ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œê°’ ì¡´ì¬)")
        print("=" * 70)
        
        y_true = df_original['PROC_EXPOSE_LOG'].values
        y_pred = predictions
        
        # RMSE
        rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
        
        # MAE
        mae = np.mean(np.abs(y_true - y_pred))
        
        # RÂ²
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - y_true.mean()) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        # ìƒëŒ€ ì˜¤ì°¨
        rel_error = rmse / y_true.mean() * 100
        
        print(f"\ní‰ê°€ ì§€í‘œ:")
        print(f"  RMSE:        {rmse:.6f}")
        print(f"  MAE:         {mae:.6f}")
        print(f"  RÂ²:          {r2:.6f}")
        print(f"  ìƒëŒ€ ì˜¤ì°¨:   {rel_error:.2f}%")
        
        # ê²°ê³¼ì— ì˜¤ì°¨ë„ ì¶”ê°€
        df_result['ERROR'] = y_true - y_pred
        df_result['ABS_ERROR'] = np.abs(y_true - y_pred)
        df_result.to_csv(output_csv_path, index=False)
        
        print(f"\nâœ“ ì˜¤ì°¨ ì •ë³´ë„ ê²°ê³¼ì— ì¶”ê°€ë¨ (ERROR, ABS_ERROR)")
    
    print("\n" + "=" * 70)
    print("ì™„ë£Œ!")
    print("=" * 70)
    
    return predictions, df_result


# ================================================================
# ë©”ì¸ ì‹¤í–‰
# ================================================================

if __name__ == "__main__":
    args = parse_args()
    
    print("\nì„¤ì •:")
    print(f"  ì…ë ¥ CSV:   {args.input_csv}")
    print(f"  ëª¨ë¸ ê²½ë¡œ:  {args.model_path}")
    print(f"  ì „ì²˜ë¦¬ ë©”íƒ€: {args.preprocessing_meta}")
    print(f"  ì¶œë ¥ CSV:   {args.output_csv}")
    print(f"  ë°°ì¹˜ í¬ê¸°:  {args.batch_size}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.input_csv).exists():
        print(f"\nâŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.input_csv}")
        exit(1)
    
    if not Path(args.model_path).exists():
        print(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.model_path}")
        exit(1)
        
    if not Path(args.preprocessing_meta).exists():
        print(f"\nâŒ ì˜¤ë¥˜: ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {args.preprocessing_meta}")
        exit(1)
    
    # ì˜ˆì¸¡ ì‹¤í–‰
    predictions, df_result = predict(
        args.input_csv,
        args.model_path,
        args.preprocessing_meta,
        args.output_csv,
        args.batch_size
    )
    
    print(f"\nâœ… ì„±ê³µ!")
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {args.output_csv}")
