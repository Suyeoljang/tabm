"""
TabM í•™ìŠµ - íƒ€ê²Ÿ í‘œì¤€í™” ë²„ê·¸ ìˆ˜ì • ë²„ì „
"""

import math
import random
from typing import Literal, NamedTuple, Optional
from copy import deepcopy

import numpy as np
import pandas as pd
import scipy.special
import sklearn.metrics
import sklearn.preprocessing
import torch
import torch.nn as nn
import torch.optim
from torch import Tensor
import pickle

# TabM ê´€ë ¨ ì„í¬íŠ¸
try:
    import tabm
    import rtdl_num_embeddings
except ImportError:
    print("âš ï¸  tabm íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

# ================================================================
# ì»¤ìŠ¤í…€ Loss Functions
# ================================================================

class LogCoshLoss(nn.Module):
    """Log-Cosh Loss: ë§¤ë„ëŸ¬ìš´ Huber"""
    def forward(self, y_pred, y_true):
        error = y_pred - y_true
        loss = torch.log(torch.cosh(error))
        return loss.mean()

# ================================================================
# ì„¤ì •
# ================================================================
LEARNING_RATE = 2e-3
WEIGHT_DECAY = 4e-3
BATCH_SIZE = 512
N_EPOCHS = 1000
PATIENCE = 100
N_BINS = 16
D_EMBEDDINGS = 16
DROPOUT = 0.1
N_BLOCKS = 4  # í˜„ì¬ 4ì—ì„œ ì¦ê°€
D_BLOCK = 512

print("=" * 70)
print("TabM í•™ìŠµ - íƒ€ê²Ÿ í‘œì¤€í™” ë²„ê·¸ ìˆ˜ì • ë²„ì „")
print("=" * 70)

# ì‹œë“œ ê³ ì •
seed = 42
random.seed(seed)
np.random.seed(seed + 1)
torch.manual_seed(seed + 2)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ë””ë°”ì´ìŠ¤: {device}")

# ================================================================
# 1. ë°ì´í„° ë¡œë“œ
# ================================================================
print("\n" + "=" * 70)
print("1. ë°ì´í„° ë¡œë“œ")
print("=" * 70)

# Train/Val ë°ì´í„° ë¡œë“œ
df_trainval = pd.read_csv('encoded_trainval_data.csv')
df_test = pd.read_csv('encoded_test_data.csv')

# ë¶„í•  ì¸ë±ìŠ¤ ë¡œë“œ
split_data = np.load('data_split.npz')
train_idx = split_data['train_idx']
val_idx = split_data['val_idx']
test_size = split_data['test_size']

# ë©”íƒ€ë°ì´í„° ë¡œë“œ
with open('preprocessing_metadata.pkl', 'rb') as f:
    metadata = pickle.load(f)

label_encoders = metadata['label_encoders']
cat_cardinalities = metadata['cat_cardinalities']
numerical_cols = metadata['numerical_cols']
categorical_cols = metadata['categorical_cols']
target_col = metadata['target_col']

print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
print(f"  íƒ€ê²Ÿ: {target_col}")
print(f"  Train: {len(train_idx):,}ê°œ")
print(f"  Val:   {len(val_idx):,}ê°œ")
print(f"  Test:  {len(df_test):,}ê°œ (ë³„ë„ íŒŒì¼)")

# Train/Val ë°ì´í„° ë¶„ë¦¬
y_trainval = df_trainval[target_col].values
X_trainval = df_trainval.drop(columns=[target_col])

# Test ë°ì´í„°
y_test = df_test[target_col].values
X_test = df_test.drop(columns=[target_col])

# ================================================================
# 2. numpy ë°°ì—´ ë³€í™˜
# ================================================================
print("\n" + "=" * 70)
print("2. numpy ë°°ì—´ ë³€í™˜")
print("=" * 70)

# Train/Valì—ì„œ numerical, categorical ë¶„ë¦¬
X_num_trainval = X_trainval[numerical_cols].values
X_cat_trainval = X_trainval[categorical_cols].values
Y_numpy_trainval = y_trainval

# Testì—ì„œ numerical, categorical ë¶„ë¦¬
X_num_test = X_test[numerical_cols].values
X_cat_test = X_test[categorical_cols].values
Y_numpy_test = y_test

data_numpy = {
    'train': {
        'x_num': X_num_trainval[train_idx].astype(np.float32),
        'x_cat': X_cat_trainval[train_idx].astype(np.int64),
        'y': Y_numpy_trainval[train_idx].astype(np.float32)
    },
    'val': {
        'x_num': X_num_trainval[val_idx].astype(np.float32),
        'x_cat': X_cat_trainval[val_idx].astype(np.int64),
        'y': Y_numpy_trainval[val_idx].astype(np.float32)
    },
    'test': {
        'x_num': X_num_test.astype(np.float32),
        'x_cat': X_cat_test.astype(np.int64),
        'y': Y_numpy_test.astype(np.float32)
    }
}

print("âœ“ ë³€í™˜ ì™„ë£Œ")
print(f"  Train: x_num {data_numpy['train']['x_num'].shape}, x_cat {data_numpy['train']['x_cat'].shape}")
print(f"  Val:   x_num {data_numpy['val']['x_num'].shape}, x_cat {data_numpy['val']['x_cat'].shape}")
print(f"  Test:  x_num {data_numpy['test']['x_num'].shape}, x_cat {data_numpy['test']['x_cat'].shape}")

# ================================================================
# 3. ë°ì´í„° ì „ì²˜ë¦¬
# ================================================================
print("\n" + "=" * 70)
print("3. ë°ì´í„° ì „ì²˜ë¦¬")
print("=" * 70)

x_num_train = data_numpy['train']['x_num']
noise = np.random.default_rng(0).normal(0.0, 1e-5, x_num_train.shape).astype(x_num_train.dtype)

preprocessing = sklearn.preprocessing.QuantileTransformer(
    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),
    output_distribution='normal',
    subsample=10**9,
).fit(x_num_train + noise)

for part in data_numpy:
    data_numpy[part]['x_num'] = preprocessing.transform(data_numpy[part]['x_num'])

# íƒ€ê²Ÿ í‘œì¤€í™” (Train, Val, Test ëª¨ë‘!)
class RegressionLabelStats(NamedTuple):
    mean: float
    std: float

task_type: Literal['regression', 'binclass', 'multiclass'] = 'regression'
n_classes = None

Y_train = data_numpy['train']['y'].copy()
regression_label_stats = RegressionLabelStats(
    Y_train.mean().item(), Y_train.std().item()
)

# ğŸ”¥ ì¤‘ìš”: Train, Val, Test ëª¨ë‘ í‘œì¤€í™”!
for part in ['train', 'val', 'test']:
    data_numpy[part]['y'] = (data_numpy[part]['y'] - regression_label_stats.mean) / regression_label_stats.std

print(f"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ")
print(f"  íƒ€ê²Ÿ mean: {regression_label_stats.mean:.6f}")
print(f"  íƒ€ê²Ÿ std:  {regression_label_stats.std:.6f}")

# í‘œì¤€í™” í™•ì¸
print(f"\nâœ“ í‘œì¤€í™” í™•ì¸:")
for part in ['train', 'val', 'test']:
    y = data_numpy[part]['y']
    print(f"  {part:5s}: mean={y.mean():7.4f}, std={y.std():7.4f}, min={y.min():7.4f}, max={y.max():7.4f}")

print("\n" + "=" * 70)
print("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! ì´ì œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print("=" * 70)

# ================================================================
# 4. PyTorch í…ì„œ ë³€í™˜
# ================================================================
data = {
    part: {key: torch.tensor(value, device=device) 
           for key, value in part_data.items()}
    for part, part_data in data_numpy.items()
}

# ================================================================
# 5. TabM ëª¨ë¸ ìƒì„±
# ================================================================
print("\n" + "=" * 70)
print("4. TabM ëª¨ë¸ ìƒì„±")
print("=" * 70)

n_num_features = len(numerical_cols)
n_cat_features = len(categorical_cols)

num_embeddings = rtdl_num_embeddings.PiecewiseLinearEmbeddings(
    rtdl_num_embeddings.compute_bins(
        torch.tensor(data_numpy['train']['x_num'], device='cpu'),
        n_bins=N_BINS
    ),
    d_embedding=D_EMBEDDINGS,
    activation=False,
    version='B',
)

model = tabm.TabM.make(
    n_num_features=n_num_features,
    cat_cardinalities=cat_cardinalities,
    d_out=1,
    dropout=DROPOUT,
    num_embeddings=num_embeddings,
    n_blocks = N_BLOCKS,  # í˜„ì¬ 4ì—ì„œ ì¦ê°€
    d_block = D_BLOCK
).to(device)

print(f"âœ“ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
print(f"  íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")

# ================================================================
# 6. í•™ìŠµ ì„¤ì •
# ================================================================
print("\n" + "=" * 70)
print("5. í•™ìŠµ ì„¤ì •")
print("=" * 70)

optimizer = torch.optim.AdamW(model.parameters(), 
                               lr=LEARNING_RATE, 
                               weight_decay=WEIGHT_DECAY)
gradient_clipping_norm: Optional[float] = 1.0
share_training_batches = True

from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode='max', 
                              factor=0.9, patience=40, 
                              min_lr=1e-7)

if device.type == 'cuda':
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    if gpu_memory_gb >= 40:
        eval_batch_size = 65536
    elif gpu_memory_gb >= 20:
        eval_batch_size = 32768
    else:
        eval_batch_size = 16384
else:
    eval_batch_size = 4096

criterion = LogCoshLoss()

print(f"í•™ìŠµ ì„¤ì •:")
print(f"  Learning rate: {LEARNING_RATE}")
print(f"  Weight decay: {WEIGHT_DECAY}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Max epochs: {N_EPOCHS}")
print(f"  N_BINS: {N_BINS}")
print(f"  D_EMBEDDINGS: {D_EMBEDDINGS}")
print(f"  Dropout: {DROPOUT}")
print(f"  Loss: LogCosh")
print(f"  N_BLOCKS: {N_BLOCKS}")
print(f"  D_BLOCK: {D_BLOCK}")

# ================================================================
# 7. í•™ìŠµ í•¨ìˆ˜
# ================================================================

def apply_model(part: str, idx: Tensor) -> Tensor:
    x_num = data[part]['x_num'][idx]
    x_cat = data[part]['x_cat'][idx]
    return model(x_num, x_cat)


def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:
    y_pred = y_pred.flatten(0, 1).squeeze(-1)
    
    if share_training_batches:
        y_true = y_true.repeat_interleave(model.backbone.k)
    else:
        y_true = y_true.flatten(0, 1)
    
    return criterion(y_pred, y_true)


@torch.no_grad()
def evaluate(part: str) -> float:
    """í‘œì¤€í™”ëœ ìŠ¤ì¼€ì¼ì—ì„œ í‰ê°€ (ì—­ë³€í™˜ ì•ˆ í•¨!)"""
    model.eval()
    
    y_pred: np.ndarray = (
        torch.cat([
            apply_model(part, idx)
            for idx in torch.arange(len(data[part]['y']), device=device).split(eval_batch_size)
        ])
        .cpu()
        .numpy()
    )
    
    # Ensemble mean (kê°œ ëª¨ë¸ í‰ê· )
    y_pred = y_pred.mean(1)
    
    # í‘œì¤€í™”ëœ ìŠ¤ì¼€ì¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©!
    y_true = data[part]['y'].cpu().numpy()
    
    # RMSE ê³„ì‚° (í‘œì¤€í™”ëœ ìŠ¤ì¼€ì¼ì—ì„œ)
    score = -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)
    
    return float(score)

# ================================================================
# 8. í•™ìŠµ ì‹œì‘
# ================================================================
print("\n" + "=" * 70)
print("6. í•™ìŠµ ì‹œì‘")
print("=" * 70)

print(f'\ní•™ìŠµ ì „ Test RMSE (í‘œì¤€í™” ìŠ¤ì¼€ì¼): {-evaluate("test"):.6f}')

best_val_score = -np.inf
best_epoch = -1
best_state = None
no_improvement_count = 0

for epoch in range(N_EPOCHS):
    model.train()
    
    epoch_losses = []
    for batch_idx in torch.randperm(len(data['train']['y']), device=device).split(BATCH_SIZE):
        optimizer.zero_grad()
        
        y_pred = apply_model('train', batch_idx)
        
        if share_training_batches:
            y_true = data['train']['y'][batch_idx]
        else:
            batch_idx_k = torch.stack([
                torch.randperm(len(data['train']['y']), device=device)[:len(batch_idx)]
                for _ in range(model.backbone.k)
            ], dim=1)
            y_true = data['train']['y'][batch_idx_k]
        
        loss = loss_fn(y_pred, y_true)
        epoch_losses.append(loss.item())
        
        loss.backward()
        
        if gradient_clipping_norm is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping_norm)
        
        optimizer.step()
    
    avg_loss = np.mean(epoch_losses)
    val_score = evaluate('val')
    test_score = evaluate('test')
    
    scheduler.step(val_score)
    current_lr = optimizer.param_groups[0]['lr']
    
    if val_score > best_val_score:
        best_val_score = val_score
        best_epoch = epoch
        best_state = deepcopy(model.state_dict())
        no_improvement_count = 0
        print(f'* [epoch] {epoch:<3} [loss] {avg_loss:.6f} [val] {val_score:.6f} [test] {test_score:.6f} [lr] {current_lr:.6f}')
    else:
        no_improvement_count += 1
        if epoch % 5 == 0:
            print(f'  [epoch] {epoch:<3} [loss] {avg_loss:.6f} [val] {val_score:.6f} [test] {test_score:.6f} [lr] {current_lr:.6f}')
    
    if no_improvement_count >= PATIENCE:
        print(f'\nEarly stopping at epoch {epoch}')
        break

# ================================================================
# 9. ìµœì¢… í‰ê°€
# ================================================================
print("\n" + "=" * 70)
print("7. ìµœì¢… í‰ê°€")
print("=" * 70)

model.load_state_dict(best_state)

final_val_score = evaluate('val')
final_test_score = evaluate('test')

# ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›í•´ì„œ ì¶œë ¥
final_val_rmse = -final_val_score * regression_label_stats.std
final_test_rmse = -final_test_score * regression_label_stats.std

print(f'\nBest epoch: {best_epoch}')
print(f'í‘œì¤€í™” ìŠ¤ì¼€ì¼:')
print(f'  Validation RMSE: {-final_val_score:.6f}')
print(f'  Test RMSE: {-final_test_score:.6f}')
print(f'\nì›ë˜ ìŠ¤ì¼€ì¼:')
print(f'  Validation RMSE: {final_val_rmse:.6f}')
print(f'  Test RMSE: {final_test_rmse:.6f}')

# ================================================================
# 10. ëª¨ë¸ ì €ì¥
# ================================================================
save_path = 'tabm_model_fixed.pt'
torch.save({
    'model_state_dict': best_state,
    'regression_label_stats': regression_label_stats,
    'preprocessing': preprocessing,
    'label_encoders': label_encoders,
    'numerical_cols': numerical_cols,
    'categorical_cols': categorical_cols,
    'cat_cardinalities': cat_cardinalities,
    'best_epoch': best_epoch,
    'best_val_score': best_val_score,
    'config': {
        'lr': LEARNING_RATE,
        'weight_decay': WEIGHT_DECAY,
        'batch_size': BATCH_SIZE,
        'n_bins': N_BINS,
        'd_embeddings': D_EMBEDDINGS,
        'dropout': DROPOUT,
    }
}, save_path)

print(f"\nâœ“ ëª¨ë¸ ì €ì¥: {save_path}")
print("=" * 70)
